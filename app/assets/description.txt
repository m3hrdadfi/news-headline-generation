This [paper](https://arxiv.org/abs/1907.12461) introduced the opportunity to use available pre-trained LMs (BERT, GPT-2, and RoBERTa) to achieve state-of-the-art results on some interesting NLP tasks like Text Generation.
So, I trained an EncoderDecoder model based on parsBERT on [VOA Persian Corpus](https://jon.dehdari.org/corpora/#persian) (a medium-sized corpus of 7.9 million words, 2003-2008) to generate headlines.
The model achieved a 25.30 ROUGE-2 score.

<br />

|    %    |   p   |   r   |   f   |
|:-------:|:-----:|:-----:|:-----:|
| rouge-2 | 24.50 | 25.30 | 24.24 |
<small>Table 1: Rouge scores obtained by the Bert2Bert model.</small>

<br />

|    #    | Train |  Dev | Test |
|:-------:|:-----:|:----:|:----:|
| Dataset | 31,550 | 3,506 | 3,896 |
<small>Table 2: Dataset information</small>